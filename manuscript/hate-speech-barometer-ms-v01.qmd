---
title: "The hate-speech barometer template: A tutorial on design and setup"
subtitle: Manuscript, V01
date: 2024-05-24
licence: CC-BY
project:
  output-dir: docs

authors: Sebastian Sauer, Alexander Piazza, Sigurd Schacht
bibliography: /Users/sebastiansaueruser/Google Drive/Literatur/refmgt/library-ses.bib
toc: true
number-sections: true
format:
  html:
    theme: journal
    embed-resources: true  
---



# Introduction

## Hate speech is a concern for cyber security, social peace, and mental health

According to the United Nations, "hate speech" can be defined as offensive discourse targeting a group or an individual based on on personal characteristics such aus race, religion, or gender^[https://www.un.org/en/hate-speech/understanding-hate-speech/what-is-hate-speech, accessed 2024-05-24]. The UN amends that hate speech may threaten social peace.
Although there is a lack of a widely accepted definition, the UN suggests the following definition of hate speech:

>    any kind of communication in speech, writing or behaviour, that attacks or uses pejorative or discriminatory language with reference to a person or a group on the basis of who they are, 
in other words, based on their religion, ethnicity, nationality, race, colour, descent, gender or other identity factor.

Although hate speech is nothing new, it has maybe been given a boost by the internet, which has made it possible for threats, conspiracies, and lies to travel quickly throughout the globe.
Hate speech is having a visible impact on society: 
there are many commonalities between the January assaults on Brazil's government buildings,
 and the attack on the US Capitol on January 6, 2021, 
including that each event happened after certain groups continuously used threatening language and false allegations against others.
According to a BBC new article, online hate speech in the UK and US has risen by approx. 20% since the start of the Covid pandemic^[https://www.bbc.com/news/newsbeat-59292509, accessed 2024-05-24].
More recently, Iceland's government is the 34th to join ratification concerning the criminalization of acts of a racist and/or xenophobic natur commited through computer systems^[https://www.coe.int/en/web/cyberviolence/-/iceland-joins-the-first-additional-protocol-to-the-convention-on-cybercrime-on-countering-xenophobic-and-racist-acts-committed-through-computer-systems, accessed 2024-05-24].
Some researchers have even put forward the hypothesis of a causal link between social media use and offline violence [@cinelli_dynamics_2021; @calvert_hate_1997; @chan_internet_2016; @chan_internet_2016].
@carley_social_2020 summarize that hate speech constitutes a major threats not only for democracy and civil rights including freedom, but also for individual mental and psychosocial health. 
For example, @wypych_psychological_2022 conducted an online survey among N=726 Ukrainian immigrants living in Poland. 
The authors aimed at investigating the association between exposure to hate speech, stress, and mental health. 
It is concluded that (prolonged) exposure to hate speech causes mental health problems of the target population. 
In sum, albeit a monetary or similar quantification is difficult, it can be concluded that hate speech is a substantial menace to society. 
It is the aim of the research presented in this paper, we fight back hate speech by fostering research endeavors for detecting hate speech.


## Present state of hate speech detection research

It is important to address hate speech to prevent violence against protected characteristics and to promote a safe and respectful online environment.
However, setting limits on speech at a global scale in various languages and cultures is complex and identifying hate speech can be difficult in an online global community.
One aspect that contributes to the difficulties in hate speech detection is that false negatives (missing hate speech) and false positive (false accusing of hate speech) are like Skylla and Charybdis the opposing monsters of the ethical consequences of such decision making, as opposing fundamental rights might clash^[https://about.fb.com/news/2017/06/hard-questions-hate-speech/, accessed 2024-05-24].
Augmenting the already high difficulties in detecting hate speech is that annotators are not necessarily reliable and a universal definition of hate speech does not exist (as to yet).


Different methodologies for detecting hate speech have been developed and are in widely circulated use, 
comprising deep learning, shallow learning and text-mining (non machine learning) approaches. 
One basic text mining approach is a keyword-based method, where an ontology or dictionary is used to identify text containing potentially hateful keywords [@macavaney_hate_2019].
However, simply using a hateful slur is not enough to constitute hate speech according to a study of different definitions of hate speech  [@macavaney_hate_2019]. 
More advanced techniques include machine learning models ranging from word count (e.g., TFIDF) methods to complex BERT models [@jahan_systematic_2021].
Successful detection models use more than one approach, including hybrid models that combine different techniques for more accurate results [@alkomah_literature_2022].
Advancements in natural language processing (NLP) and machine learning have greatly improved the detection of hate speech. With the help of machine learning algorithms, particularly deep neural networks, NLP can be used to identify linguistic patterns and features that are indicative of hate speech [@jahan_systematic_2021; @pang_deep_2022; @yin_towards_2021; @velankar_review_2022].
Various approaches have been used to detect specific features or linguistic patterns that denote hate speech in text, 
including rule-based classification models and, more recently, a proliferation of deep learning methods like Long Short-Term Memory networks (LSTM) and Transformer-based architectures [@malik_deep_2022]. 

Whereas hate speech detection is an active field of investigation, the borders between other form of social behavior is blurry.
 For example, bot detection is an emerging research (and engineering) branch that has sparked a substantial number of research activities. 
For a research overview, see @cresci_decade_2020.


## High-level ideas of machine learning

Machine learning (ML) is often considered as a subset of artificial intelligence (AI). AI is a broad field that aims to emulate human abilities, 
while machine learning focuses on training a machine to learn and adapt through experience [@angra_machine_2017, @bakshi_considerations_2018].
ML constitutes the intersection between statistics and computer science, and its rapid progress habe largely been driven by the ongoing reduction in computational costs [@jordan_machine_2015].
In its score, ML is a new interpretation of the old quest of finding patterns in data. 
Correlations, which have been a subject of statistical studies at least since a century, are among its most prototypical examples. 
Once pattern have been found in the data, predictions can be inferred.
The action of reducing a data set with many variables to a (potentially very) simple association rule, 
is what has been dubbed a "model" [@stigler_seven_2016].
In fact, the usefulness of a model hinges on its ability to be reductive.
To be clear, there is not causal knowledge necessary in order for some model to predict some event,
which probably fuelled its widespread use given the fact that causal knowledge is very hard to gain, 
and surpasses a purely statistically oriented research agenda [cf. @pearl2009causality].
ML algorithms of the present day are highly flexible allowing to "fit an elephant", 
as von Neumann remarked to a similar matter^[https://math.stackexchange.com/questions/2970219/was-von-neumann-right-that-with-four-parameters-you-can-fit-an-elephant, accessed 2024-05-24].
On the pro side, highly flexible algorithms are able to pick up even minute and complex patterns in data, which may in many circumstances be useful as many phenomena, particularly in the social sciences, tend to behave in complex ways.
However, there are drawbacks of highly flexible algorithms as well:
Such algorithms tend to perceive signals where in fact there is only noise,
a phenomenon well known as Pareidolia in perception research, and as overfitting in ML.
To be fair, one may argue that human suffer from Pareidolia at least as much as machines do.
However, countermeasures against overfitting are in place.
To common procedures are (1) testing the model's predictions on new data, data unknown to the model, 
and (2) "prune" or "penalize" the model for complexity, 
in order to strike a balance between unnecessary complexity and exaggerated parsimony [@james_introduction_2021].


## Research gap and research objectives

Given the societal impact of hate speech and the vibrating progression in machine learning, 
applied researchers need tools and templates to enable them to investigate social science research question.
Even without being experts in machine learning, social scientists need access to the state-of-the-art tools.
This research aims at supporting this by providing a template for hate speech detection.
Our intended audience are social scientist with an intermediate technical knowledge in statistics and data science.
Luckily, typical ML pipelines are, at least in their basic form, quite mechanical and simple,
and can hence quite easily be automated.
Given the prosperity and rapid progress seen in ML it would however be ill-suited to provide polished point-and-click interfaces.
Rather, script based approaches to hate speech ML pipelines are of advantage as they can be adapted to novel development quickly.
In fact, most novel developments in statistics and ML take place, at least in the last couple of years, in the R and Python programming languages.
For that reason, we provide a template that makes use of the R language and its rich ecosystem of statistics and ML tools.
Our goal is facilitate applied researchers from the social sciences to conduct their own hate speech analyses without worrying to much about the intricate technicalities of ML.


# Research design

This paper describes a tool that facilitates a classical ML pipeline focused on hate speech detection.
The source code can be accessed freely [here](https://github.com/sebastiansauer/hate-speech-barometer).
Towards that end, a typical ML pipeline is provided including well-known steps such as tuning different ML algorithms and employing resampling schemes.
Results of the analyses as facilitated by this project could be summarized with plots such @fig-sentiplot2.

![Hate speech proxies based on Tweets to German politicians](senti_plot2.png){#fig-sentiplot2}




## Reproducibility

Reproducibility has been dubbed a hallmark of science [@plesser_reproducibility_2018], 
and the present research strongly builds on this idea.
The analysis is purely based on scripts which are accessible and permissively licenced (GNU General Public Licence).
Git was used as a versioning tool so that all changes to the code base can be made explicit.
A package management tool (renv; @ushey_renv_2023) is used so that users can rest assured that the correct version of the R packages needed are in place.
The train- and test-sample are openly available [cf. @germeval] so that comparing own results to the results of the tool presented here are easily achievable^[https://heidata.uni-heidelberg.de/dataset.xhtml?persistentId=doi:10.11588/data/0B5VML].


## State of the Art shallow learner via "Tidymodels"

We used the "Tidymodels" [@kuhn_tidymodels_2020] framework as an ML API.
Tidymodels builds in turn on the "Tidyverse" idea [@wickham_welcome_2019],
an opinionated approach which tries to strike a balance between being powerful enough to create high-performance models, but, on the other hand, easy to use appropriately. 
The authors state that its [the Tidyverse's] "primary goal is to facilitate a conversation between a human and a computer about data" [@wickham_welcome_2019, p. 1]. 
One advantage of any (good) approach that is widely accepted is that it provides a standard for how things should get done.
Maybe the reason of the success of the Tidyverse is that it addressed key problems of data practitioners and finds sensible balances between conflicting goals. 
In short, the authors describe their design principles as (a) human centeredness,
 meaning that the software is meant for humans to be read and written, 
and secondly only for computers to be run, similar to literate programming [@knuth_literate_1984],
 (b) consistency, so that all functions work in a similar manner, 
(c) additivity, allowing to solve complex problems by breaking them down into small pieces, and (d) inclusivity, allowing the community to participate in the development.
A more thorough introduction to Tidymodels is presented by @silge_tidy_2022.

Tidymodels support a broad variety of features which encompass recent requirements to ML software. Most prominently is the unified API for all ML algorithms and its complete coverage of all (typical) ML steps.

 For example, Tidymodels allows for intelligent tuning grid search methods such as simulated annealing.
It features outer and inner loops in cross validation and includes the preprocessing into cross validation (such as tuning the number of components in a PCA).
It provides many steps that makes data preprocessing simple such as dummyfying nominal variables, effect-coding them or over-/undersample their levels in a case of a class imbalance. 
Due to the rich ML ecosystem in R, from which many ML algorithms are made available in Tidymodels, users can choose from a wide array of state-of-the-art methods.

## Project management via "Targets"

It has been said that maybe the most difficult problem in computer science is naming object^[https://stackoverflow.com/questions/33497879/why-is-the-hardest-part-of-programming-is-naming-things].
Then maybe the next most difficult thing is dealing with complexity, at least seen from a helicopter view. 
Given a set of 10 possible actions, where you have to pick the correct 3 to solve a problem leaves you with 120 possibilities (as combinatoric math demand). 
However given a of 20 actions where you again have to pick 3 confronts you with 120 possible combinations (3 out of 30: 4060). 
In a nutshell: there's an explosion of complexity going on.
Even a moderate increase in possible actions can increase dramatically the set of possible combinations to choose from.
The bad news is there's no way out, thank math for its crisp purity. 
The good news is the only thing to do is to reduce complexity to the level just low enough to just manage it.
That's where project management comes into play.
Project management in software development has many aspects;
one very well-known idea is "don't repeat yourself" (DRY), which could be translated in using macros (functions) to avoid repetition in code [@hunt_pragmatic_2000].
One key feature of R is its functional programming orientation, which allows code to be neatly composable.
The project management tool used in this project is called "targets" which is build around functional programming ideas [@landau_targets_2021].
It is a GNU-Make-like pipeline toolkit for R.
Just like Make, Targets take care that the objects of a pipeline are updated if and only if necessary. That means if an "upstream" objects changes and if this object is an input to a downstream object, than (and only then) the downstream object needs to be updated.
In the light of heavy computation costs, it can be vital to know when updating is not needed. 
On the other hand, it is similarly important not to miss updating if in fact it is outdated.
In a nutshell, Targets as a project management toolkit (a) updates objects of a pipelines, and (b) keeps the pipeline neat and clean.
An example is provided below.



# ML pipeline of the hate speech barometer

@fig-pipe3 shows the pipeline of the hate speech barometer;
an interactive version of the diagram is available [here](https://sebastiansauer.github.io/hate-speech-barometer/tar-visnetwork-pipeline3.html). 
In this graph, each node describes a target and each edge shows dependencies between the targets with the arrows heading downstream.
The appendix provides an overview in tabular form of the targets of the ML pipeline (@tbl-targets).


![pipeline of the hate speech barometer](../img/pipeline3.png){#fig-pipe3}

In the following, we describe the steps of the pipeline in some detail so that practitioners know what each step accomplishes.
Instead of a "step" the term "target" could be used when seen from a functional programming view, focusing on the value (or output, result) of a function.
For each step (or target) of the pipeline, we provide its name as used in the code along with a short description of what is achieved by the step.
Where the step is complex enough to merit its own function, we provide the URL to the function^[Please note that the URLs to the functions are still subject to change as the project is an early development phase.].


## Data preprocessing


- `path`: Defines the (relative) paths to the data^[https://github.com/sebastiansauer/hate-speech-barometer/blob/main/R/set-path.R]
- `d_train` and `d_test`: Imports train- and test-sample (based on the paths, that's why the object `path` is the input for this target).^[https://github.com/sebastiansauer/hate-speech-barometer/blob/main/R/read-test-data.R]
- `recipe2`, `recipe2_prepped`, `d_train_baked`, `d_test_baked`: Defines the preprocessing "recipe" of the data (prior to modelling) and applies it to the data sets.^[https://github.com/sebastiansauer/hate-speech-barometer/blob/main/R/def-recipes.R]
- `recipe_plain`, `recipe_plain_prepped`: As the data preprocessing was time consuming and had no tuning parameters it was taken out from the model workflow (and the cross validation) and conducted before the modelling, in order to save computation time. During the modelling workflow, a minimal preprocessing took place.


## Modelling


- `model_lasso`, `model_boost`, `model_rf`: Three learning algorithms, i.e., the Lasso (L1 penalized regression), gradient boosting, random forests^[https://github.com/sebastiansauer/hate-speech-barometer/blob/main/R/def-models.R]
- `wf1`, `wf2`,  `wf3`: In Tidymodels, a workflow (wf) consists of preprocessing and the ML algorithm plus optional postprocessing. `wf1` ist the workflow consisting of the Lasso as ML algorithm; `wf2`: boosting, `wf3`: random forest. The preprocessing was identical in all three workflows.^[https://github.com/sebastiansauer/hate-speech-barometer/blob/main/R/tune-wf.R]
- `wf1_fit` etc.: The cross-validated, tuned workflow, i.,e., the fitted model, where the model parameters have been estimated^[https://github.com/sebastiansauer/hate-speech-barometer/blob/main/R/tune-wf.R]
- `wf1_autoplot` etc.: Diagrams depicting model performance (mean and sd) according to the selected performance measures (ROC-AUC in this case)
- `wf_fits_l`, `wf_fits_roc`, `wf_fits_best`: All models stored in a list-object (`wf_fits_l`) in order to render access to the model performance simple; `wf_fits_roc` contains all the performance measures (ROC-AUC), and `wf_fits_best` identifies the model exhibiting the best fit.
- `wf3_finalized`, `final_fit`, `preds_test`: The best performing workflow is chosen and initiated with the best performing tuning parameters (`wf3_finalized`), then, the whole train sample is fit with the best model, giving `final_fit`; on this base the test sample is predicted (`preds_test`).


### Tweet classification

- `tweets_path`: A folder containing tweet data; in case of changes in the folder the target will be updated
- `tweets`, `tweets_df`: The tweets are imported into R (`tweets`) using parallel processing due to large size; in order to save space and time, a random sample of tweets can be drawn (`tweets_df`). 
- `tweets_baked`: The tweets are subjected to the same preprocessing as the train sample.
- `preds`, `tweets_baked_preds`: The tweets get classified (predicted) and the predictions are added as an extra column to the processsed tweets (`tweets_data_preds`).


### Pipeline results


- `preds_summarized`: Proportion of hate speech per Twitter account, per year, s. Fig. ^[https://github.com/sebastiansauer/hate-speech-barometer/blob/main/R/helper-funs.R]

- `preds_summarized_plot`: Plot for `preds_summarized`^[https://github.com/sebastiansauer/hate-speech-barometer/blob/main/R/plots.R]


## Constants

Any ML pipelines depends on some constants, for organizational reasons (e.g., paths) or for model (hyper)parameters, to name two usual suspects.
In this project, users can easily change their configuration in a (yaml) text file called `config.yml`.
Advanced users can make use of the different branches provided by Git;
whereas the main branch provides the standard pipelines,
the "dev" branch offers more pipelines and experimental features.


# Limitations

As this project is an early development phase, there are several threads suitable for further buildup. 
For example, the documentation of the project is still large lacking,
which renders the access more difficult for less advanced users.
In addition, deep learning methods are not yet implemented (although planned).
Of course, users of any technical system follow two opposing goals: feature richness and simplicity. 
The optimal balance between the two goals partly depends on the user's background and goals.
That said, this project draws from an array of tools which implies that the user is accustomed to these tools (R, Git, Github, Targets, Tidymodels).
Limited knowledge of these tools will place a barrier to easy access to the system.
Working with text data can place substantial burden on the computational resources.
As to yet the present tool is not yet fully optimized to saving resources.

There is a substantial number of case studies and tutorial on ML pipelines freely available on the web.
However, there's still, to the best of our knowledge no case study or template for a complex data analysis incorporating Tidymodels and Targets.

In sum, it is our hope that the present research contributes to the detection of hate speech by providing a scaffold to the applied research so that he or she can focus on the phenomenon of hate speech rather on the technical intricacies of ML.



# Appendix {.appendix}

@tbl-targets provides an overview of the targets of the analytic pipeline of the hate speech barometer.

```{r}
#| echo: false
#| tbl-cap: Overview of the targets of the pipeline
#| label: tbl-targets
d <- readr::read_rds("manifest_pipeline3.rds") |>
  dplyr::select(1,2) 

d |> gt::gt()
```



